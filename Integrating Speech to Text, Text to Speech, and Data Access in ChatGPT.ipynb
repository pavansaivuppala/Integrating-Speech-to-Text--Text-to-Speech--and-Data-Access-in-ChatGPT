{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tya8aaeZPVfa",
        "outputId": "24e7e045-6045-4fe9-82cf-0334221e0f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q openai\n",
        "!pip install -q gTTS\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q openai\n",
        "!pip install -q langchain\n",
        "!pip install -q unstructured\n",
        "!pip install -q pdfminer.six"
      ],
      "id": "tya8aaeZPVfa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad66c9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d48ed5d-1666-454b-e7d7-e345a1fe4a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import os\n",
        "import nltk\n",
        "import openai\n",
        "import langchain\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\")"
      ],
      "id": "ad66c9aa"
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "pdf_path = '/content/resum_pavan.pdf'  # Replace with the actual path to your PDF file\n",
        "\n",
        "# Extract text from the entire PDF\n",
        "pdf_text = extract_text(pdf_path)\n"
      ],
      "metadata": {
        "id": "PGMST8bV7Gku"
      },
      "id": "PGMST8bV7Gku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/dynamic_file.txt'  # Replace with the desired file path and name\n",
        "dynamic_content = pdf_text\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(dynamic_content)\n",
        "file_path = '/content/drive/My Drive/dynamic_file.txt'\n"
      ],
      "metadata": {
        "id": "Rp0TnLFh8gBe"
      },
      "id": "Rp0TnLFh8gBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('/content/', glob='**/*.txt')"
      ],
      "metadata": {
        "id": "pXpGGoC_6NA6"
      },
      "id": "pXpGGoC_6NA6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a9740d9",
        "outputId": "049f6921-9897-48e4-96ee-c16c9c2b0880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Load up your text into documents\n",
        "documents = loader.load()"
      ],
      "id": "6a9740d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3153f864"
      },
      "outputs": [],
      "source": [
        "# Get your text splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
      ],
      "id": "3153f864"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a792c6fb"
      },
      "outputs": [],
      "source": [
        "# Split your documents into texts\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "id": "a792c6fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2cad0de"
      },
      "outputs": [],
      "source": [
        "# Turn your texts into embeddings\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
      ],
      "id": "d2cad0de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "734ed265"
      },
      "outputs": [],
      "source": [
        "# Get your docsearch ready\n",
        "docsearch= FAISS.from_documents(texts, embeddings)"
      ],
      "id": "734ed265"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66826924",
        "outputId": "030e96b5-d270-429a-f9c8-720bca8d7015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load up your LLM\n",
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\")"
      ],
      "id": "66826924"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "817a0ece"
      },
      "outputs": [],
      "source": [
        "# Create your Retriever\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "id": "817a0ece"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "20b81063",
        "outputId": "81cb7714-ad1e-4f33-afa5-81a7fde636ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pavan Sai Vuppala\\n\\nEmail: pavansai@gmail.com\\n\\nAI Engineer in Client Engineering\\n\\nProfessional Summary:\\nHighly skilled AI Engineer with expertise in Foundation Models and building AI systems. Experienced in fine-tuning pre-trained models and collaborating with stakeholders to implement AI solutions. Proficient in utilizing IBM WatsonX platform and applying prompt-engineering techniques. Strong problem-solving and communication skills.\\n\\nTechnical Skills:\\nSkill Category         | Skills\\n------------------------------------\\nAI Development         | Python, PyTorch, NumPy, Pandas\\nCloud Technologies      | Kubernetes, AWS\\nModel Building         | Fine-Tuning, API Integration\\nStatistical Analysis    | Statistical Analysis, Experimental Methods\\nCommunication         | Verbal and Written English Proficiency\\n\\nProfessional Experience:\\n\\nClient: Tata Consultancy Services\\nRole: Assistant Systems Engineer\\nLocation: Hyderabad, IN\\nDates: 06/2021 - 01/2022\\n\\n• Designed a model to summarize the text and generate profiling output.\\n• Developed a clickbait detection model using fine-tuned large language model with 99.5% accuracy.\\n• Utilized Python on Google Colab to access OpenAI API for fine-tuning.\\n• Published research poster on \"Fine-Tuned Large Language Model for Improved Click-Bait Title Detection\" in TAMUCC 2023 symposium.\\n• Conducted research on lang chain framework and built a model for phishing detection.\\n\\nClient: BSNL,RTTC\\nRole: Intern\\nLocation: Hyderabad, IN\\nDates: 07/2019\\n\\n• Designed a video call application for desktop and mobile platforms using Android Studio.\\n• Ensured a seamless user experience for video calls within a secure network.\\n\\nEducation:\\n\\nMaster of Computer Science\\nTexas A&M University, Corpus Christi\\nGPA: 3.8/4.0\\n\\nBachelor of Technology in Computer Science and Engineering\\nJawaharlal Nehru Technological University, India\\n\\nCertifications:\\n- IBM WatsonX Certification in Foundation Models\\n\\nProfessional Projects:\\n\\nClient: AT&T, OH\\nRole: Senior Java Full Stack Developer\\nDates: Jan 2022 - Present\\n\\n• Developed functionalities using Agile Scrum Methodology.\\n• Extensive experience in Agile methodologies like Test Driven Development (TDD).\\n• Worked on the creation of custom Docker container images, tagging, pushing images, and integrating Spring Boot.\\n• Developed Java modules implementing business rules and workflows using Spring Boot.\\n• Maintained and expanded AWS infrastructure.\\n\\nAdditional Information:\\n- Participated in The 25th International Conference on Artificial Intelligence (ICAI\\'23).\\n- Strong contribution record with publications in top peer-reviewed scientific conferences.\\n- Proficient in utilizing large language models such as Langchain and HuggingFace Model.\\n\\nReferences:\\nAvailable upon request.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Run a query\n",
        "query = \"\"\"Based on the given job role, rewrite the given resume according to the job role and add any relvant sections or content, if needed. The job role is given as:\n",
        " $$$Your Role and Responsibilities\n",
        "\n",
        "An AI Engineer in Client Engineering is a specialist in Foundation Models and building AI systems. You’ll leverage the Watson X platform with clients to co-create the value AI can bring to business. You will focus on Technology Patterns as our way to build the right skills and assets for repeatability to delight clients. AI Engineers work in high touch environment on all watsonx.ai opportunities.\n",
        "\n",
        "We’re passionate about success. If this role is right for you, then your achievements will mean that your career is flourishing, your team is succeeding, and our clients are thriving. To help ensure this win-win outcome, a ‘day-in-the life’ of this opportunity may include, but not be limited to:\n",
        "\n",
        "Fine Tuning pre-trained Foundation Models and assisting different stakeholders with analysis and implementation.\n",
        "Apply prompt-engineering techniques to specific use cases and types of models.\n",
        "Collaborating with the client to rapidly develop AI solutions using IBM watsonx Platform via a Proof of Experience (PoX)\n",
        "Applying Foundation Models to propose and validate hypotheses for business queries​\n",
        "Utilizing the latest IBM AI technical strategies and sales plays to unlock client opportunities, applying skills in AI development, model building, Fine-Tuning and API integration​.\n",
        "Displaying a growth mindset and genuine curiosity to grasp clients’ business processes and challenges, employing statistical analysis skills to identify transformation opportunities​.\n",
        "\n",
        "Required Technical and Professional Expertise\n",
        "\n",
        "Understanding of key concepts in the Foundation Models literature and expertise in building and deploying them for real-world examples.\n",
        "Knowledge of cloud technologies, specifically Kubernetes, and expertise in leveraging them for large-scale AI workloads.\n",
        "Ability to identify fundamental problems from real-world cloud use-cases and to design, build and validate successful AI solutions.\n",
        "Capability to demonstrate and evaluate AI solutions via experimental methods, particularly through hands-on creation of prototypes.\n",
        "Strong communication skills and the ability to collaborate effectively within a local team.\n",
        "Excellent command of the English language, both verbal and written.\n",
        "\n",
        "Preferred Technical and Professional Expertise\n",
        "\n",
        "Strong contribution record with either publications in top peer-reviewed scientific conferences and journals or strong leadership track-record in opens source communities, with a particular focus on foundation models, or large scale machine learning models.\n",
        "Track record of being part of highly collaborative teams to tackle important problems which produce high impact solutions\n",
        "Knowledge of Large Language Models (LLM), Langchain and HuggingFace Model\n",
        "$$$ and the resume format must be after name and email 1. professional summary, 2. Technical Skills: where in a table one column is skill category and other column is skills for skills and later 3. professional experience : ***Client: AT&T, OH                                                                                                           Jan 2022 - TILL DATE\n",
        "Role: Senior Java Full Stack Developer\n",
        "Responsibilities:\n",
        "•\tDeveloped the functionalities using Agile Scrum Methodology.\n",
        "•\tExtensive experience in various Agile methodologies like Test Driven Development (TDD)\n",
        "•\tWorked on creation of custom Docker container images, tagging, pushing images, integration of Spring boot.\n",
        "•\tDeveloped java modules implementing business rules and workflows using Spring Boot\n",
        "•\tMaintained and expanded AWS (Cloud Services) infrastructure using *** and at last education background section \"\"\"\n",
        "qa.run(query)"
      ],
      "id": "20b81063"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "694343cb"
      },
      "outputs": [],
      "source": [
        "def q(m):\n",
        "  qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "  template= m\n",
        "  r=template\n",
        "  result1 = qa({\"query\": r})\n",
        "  result = result1['result']\n",
        "  return result"
      ],
      "id": "694343cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqwZwjHhRQGI"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import time\n",
        "import openai\n",
        "import warnings\n",
        "import os"
      ],
      "id": "MqwZwjHhRQGI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj3CjqUORUV_",
        "outputId": "b2ff2cff-50fb-4af6-b703-61d818c6e56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 45.3MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model1 = whisper.load_model(\"base\")"
      ],
      "id": "Tj3CjqUORUV_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FUO59EQd2tJ"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS"
      ],
      "id": "0FUO59EQd2tJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uy-34zvQ2uP"
      },
      "outputs": [],
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model1.device)\n",
        "    _, probs = model1.detect_language(mel)\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model1, mel, options)\n",
        "    m = result.text\n",
        "    resp = q(m)\n",
        "    out_result = resp\n",
        "    return [m, out_result]"
      ],
      "id": "4Uy-34zvQ2uP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "xjj7gsbURF7X",
        "outputId": "f4adc13a-be1b-428d-b0da-87ac940fb0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-d4d50d9b017c>:9: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
            "<ipython-input-24-d4d50d9b017c>:9: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "\n",
        "\n",
        "output_1 = gr.Textbox(label=\"Speech to Text\")\n",
        "output_2 = gr.Textbox(label=\"LLM Output\")\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    title = 'Detection of clickbait using gpt-4',\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
        "    ],\n",
        "\n",
        "    outputs=[\n",
        "        output_1,  output_2\n",
        "    ],\n",
        "    live=True).launch()"
      ],
      "id": "xjj7gsbURF7X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "Iq9NcTnnfTbJ",
        "outputId": "e1bdb3e9-6db2-4a01-8dd8-80c6bb2f0711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-1fce01909529>:29: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  output_1 = gr.outputs.Textbox(label=\"Speech to Text\")\n",
            "<ipython-input-25-1fce01909529>:30: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  output_2 = gr.outputs.Textbox(label=\"LLM Output\")\n",
            "<ipython-input-25-1fce01909529>:31: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  output_audio = gr.outputs.Audio(label=\"Output Audio\", type=\"filepath\")\n",
            "<ipython-input-25-1fce01909529>:37: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
            "<ipython-input-25-1fce01909529>:37: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "from gradio.components import File\n",
        "\n",
        "\n",
        "def transcribe(audio):\n",
        "    # Your transcription logic here\n",
        "    # This function should return the transcribed text, LLM output text, and audio output path\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model1.device)\n",
        "    _, probs = model1.detect_language(mel)\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model1, mel, options)\n",
        "    m = result.text\n",
        "    resp = q(m)\n",
        "    out_result = resp\n",
        "\n",
        "    # Convert the transcribed text and LLM output to speech\n",
        "    tts_transcribed = gTTS(text=out_result, lang='en')\n",
        "    tts_transcribed.save('transcribed_output.mp3')\n",
        "\n",
        "    return m, out_result, 'transcribed_output.mp3'\n",
        "\n",
        "output_1 = gr.outputs.Textbox(label=\"Speech to Text\")\n",
        "output_2 = gr.outputs.Textbox(label=\"LLM Output\")\n",
        "output_audio = gr.outputs.Audio(label=\"Output Audio\", type=\"filepath\")\n",
        "\n",
        "gr.Interface(\n",
        "    title='Integrating Speech to Text, Text to Speech, and Data Access in ChatGPT',\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "\n",
        "    ],\n",
        "    outputs=[\n",
        "        output_1, output_2, output_audio\n",
        "    ],\n",
        "    live=True\n",
        ").launch()\n"
      ],
      "id": "Iq9NcTnnfTbJ"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}